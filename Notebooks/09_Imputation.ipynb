{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163cd362",
   "metadata": {},
   "source": [
    "### Imputation Rationale\n",
    "\n",
    "**Do not impute inconsistent/partial variables by default.** Only consider imputation if the variable is conceptually indispensable and FMI suggests the information can be credibly recovered (e.g., plausible MAR with auxiliary predictors).\n",
    "\n",
    "It’s not reasonable to impute inconsistent/partial variables without first considering FMI and context. Imputation is not a neutral operation; it encodes assumptions about the missingness mechanism, temporal comparability, and the meaning of the variable. If a variable is inconsistent across months/years, imputing it can fabricate continuity that wasn’t in the data, undermining factor analysis and comparability across regions and time.\n",
    "\n",
    "**Tier 1 — Consistent variables:**\n",
    "\n",
    "- Action: Eligible for imputation.\n",
    "- Rule: Use FMI to determine imputation intensity (light/cautious/advanced).\n",
    "- Justification: Stable measurement; imputation supports matrix completion for EFA.\n",
    "\n",
    "**Tier 2 — Partial variables (intermittent presence or minor coding drift):**\n",
    "\n",
    "- Action: Conditional imputation.\n",
    "- Rule: Impute only if FMI is moderate/high but MAR plausibility exists via auxiliary predictors, and coding is harmonized; otherwise flag for sensitivity analysis.\n",
    "- Justification: Limited comparability; treat as supporting evidence, not core FA inputs.\n",
    "\n",
    "**Tier 3 — Inconsistent variables (structural changes, major coding breaks):**\n",
    "\n",
    "- Action: Do not impute for FA.\n",
    "- Rule: Document and retain for diagnostics; consider future harmonization projects or use in qualitative context.\n",
    "\n",
    "- Justification: Imputation would manufacture comparability and can distort factor structure.\n",
    "\n",
    "**Override - Conceptual indispensability:**\n",
    "\n",
    "- Action: If a variable is central to sensitivity/resilience/exposure and lacks a close proxy, allow imputation even if partial, but only with:\n",
    "- Explicit MAR argument using auxiliary variables,\n",
    "- complete coding evidence, and\n",
    "- Sensitivity analyses comparing included vs excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116b65f",
   "metadata": {},
   "source": [
    "**Why imputing inconsistent variables without FMI review is not defensible?**\n",
    "\n",
    "Measurement instability:  \n",
    "\n",
    "Inconsistent variables often arise because the survey question changed, coding shifted, or the variable wasn’t asked in some rounds. Imputing them blindly assumes the missingness is random noise, when in fact it reflects structural differences. That creates false comparability across years.\n",
    "**Factor analysis assumptions:**\n",
    "\n",
    "FA assumes each variable measures the same construct across all observations. If a variable is inconsistent, imputing values fabricates continuity that wasn’t there. This risks producing spurious factors that look “interpretable” but are actually artifacts of imputation.\n",
    "\n",
    "**Auditability and thesis defense:**\n",
    "\n",
    "The approved pipeline methodology emphasizes transparency and conceptual justification. If the team imputes inconsistent variables without FMI, reviewers can easily challenge: “Why did you treat structurally missing data as if it were random?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05331405",
   "metadata": {},
   "source": [
    "### Documentation and audit trail\n",
    "\n",
    "Action matrix: For each variable, store:\n",
    "\n",
    "- Tag: consistent/partial/inconsistent.\n",
    "- FMI bucket: Low/Moderate/High/Critical.\n",
    "- Dimension role: sensitivity/resilience/exposure.\n",
    "- Decision: keep, impute (light/cautious/advanced), sensitivity-only, exclude from FA.\n",
    "- Rationale: conceptual indispensability, MAR plausibility, harmonization status, auxiliary predictors.\n",
    "- Sensitivity analysis flags: Flag variables where inclusion materially changes factor loadings or KMO/Bartlett results, so the team can revisit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fd194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09_Imputation Notebook — Decision Matrix Builder\n",
    "# ------------------------------------------------\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Load config ---\n",
    "with open(Path(\"./data/interim/config.json\")) as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "BASE_PATH = Path(cfg[\"BASE_PATH\"])\n",
    "INTERIM_DIR = Path(cfg[\"INTERIM_DIR\"])\n",
    "PROCESSED_DIR = Path(cfg[\"PROCESSED_DIR\"])\n",
    "LOG_DIR = Path(cfg[\"LOG_DIR\"])\n",
    "MONTH_ORDER = cfg[\"MONTH_ORDER\"]\n",
    "\n",
    "# --- Load inventory (optional, for parity) ---\n",
    "with open(Path(INTERIM_DIR) / \"inventory.json\") as f:\n",
    "    inventory = json.load(f)\n",
    "\n",
    "# --- Paths ---\n",
    "RENAMED_ROOT = BASE_PATH / \"NEW Renamed Fully Decoded Surveys\"\n",
    "CONSISTENCY_ROOT = BASE_PATH / \"NEW Variable Consistency Check\"\n",
    "FMI_ROOT = BASE_PATH / \"NEW FMI Reports\"\n",
    "DECISION_ROOT = BASE_PATH / \"Decision Matrix for Imputation\"\n",
    "os.makedirs(DECISION_ROOT, exist_ok=True)\n",
    "\n",
    "# --- Load inputs ---\n",
    "consistency_df = pd.read_csv(CONSISTENCY_ROOT / \"consistency_profile.csv\")\n",
    "fmi_df = pd.read_csv(FMI_ROOT / \"fmi_profile.csv\")\n",
    "\n",
    "# --- Merge consistency + FMI ---\n",
    "decision_df = fmi_df.merge(\n",
    "    consistency_df[[\"Variable\", \"ConsistencyTag\"]],\n",
    "    on=\"Variable\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- Handle duplicate ConsistencyTag columns if present ---\n",
    "if \"ConsistencyTag_x\" in decision_df.columns and \"ConsistencyTag_y\" in decision_df.columns:\n",
    "    decision_df[\"ConsistencyTag\"] = decision_df[\"ConsistencyTag_x\"].combine_first(decision_df[\"ConsistencyTag_y\"])\n",
    "    decision_df.drop(columns=[\"ConsistencyTag_x\", \"ConsistencyTag_y\"], inplace=True)\n",
    "\n",
    "# --- Manual factor formation dictionary (customizable) ---\n",
    "dimension_map = {\n",
    "    # Sensitivity\n",
    "    \"Available for Work\": \"Sensitivity\",\n",
    "    \"C13-Major Occupation Group\": \"Sensitivity\",\n",
    "    \"C14-Primary Occupation\": \"Sensitivity\",\n",
    "    \"C15-Major Industry Group\": \"Sensitivity\",\n",
    "    \"C16-Kind of Business (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"C24-Basis of Payment (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"C25-Basic Pay per Day (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"Class of Worker (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"Nature of Employment (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"Total Hours Worked for all Jobs\": \"Sensitivity\",\n",
    "    \"Work Arrangement\": \"Sensitivity\",\n",
    "    \"Work Indicator\": \"Sensitivity\",\n",
    "    # Resilience\n",
    "    \"C03-Relationship to Household Head\": \"Resilience\",\n",
    "    \"C04-Sex\": \"Resilience\",\n",
    "    \"C05-Age as of Last Birthday\": \"Resilience\",\n",
    "    \"C06-Marital Status\": \"Resilience\",\n",
    "    \"C07-Highest Grade Completed\": \"Resilience\",\n",
    "    \"C08-Currently Attending School\": \"Resilience\",\n",
    "    \"C09-Graduate of technical/vocational course\": \"Resilience\",\n",
    "    \"C09a - Currently Attending Non-formal Training for Skills Development\": \"Resilience\",\n",
    "    \"Household Size\": \"Resilience\",\n",
    "    # Exposure\n",
    "    \"Province\": \"Exposure\",\n",
    "    \"Province Recode\": \"Exposure\",\n",
    "    \"Region\": \"Exposure\",\n",
    "    \"Urban-RuralFIES\": \"Exposure\",\n",
    "    \"Location of Work (Province, Municipality)\": \"Exposure\",\n",
    "    \"Survey Month\": \"Exposure\",\n",
    "    \"Survey Year\": \"Exposure\",\n",
    "}\n",
    "\n",
    "# --- Dimension assignment function ---\n",
    "def assign_dimension(var):\n",
    "    if var in dimension_map:\n",
    "        return dimension_map[var]\n",
    "    v = var.lower()\n",
    "    if any(k in v for k in [\"occupation\", \"work\", \"employment\", \"job\", \"hours\", \"basis\", \"industry\"]):\n",
    "        return \"Sensitivity\"\n",
    "    elif any(k in v for k in [\"grade\", \"school\", \"household\", \"age\", \"marital\", \"ethnicity\", \"training\"]):\n",
    "        return \"Resilience\"\n",
    "    elif any(k in v for k in [\"region\", \"province\", \"urban\", \"survey\", \"weight\", \"psu\", \"replicate\"]):\n",
    "        return \"Exposure\"\n",
    "    else:\n",
    "        return \"Unclassified\"\n",
    "\n",
    "decision_df[\"Dimension\"] = decision_df[\"Variable\"].apply(assign_dimension)\n",
    "\n",
    "# --- SuggestedAction logic ---\n",
    "def suggest_action(row):\n",
    "    fmi = row[\"OverallFMI\"]\n",
    "    tag = row[\"ConsistencyTag\"]\n",
    "\n",
    "    if pd.isna(fmi):\n",
    "        return \"review\"\n",
    "    if tag == \"consistent\":\n",
    "        if fmi < 0.05: return \"keep\"\n",
    "        elif fmi < 0.20: return \"impute_light\"\n",
    "        elif fmi < 0.40: return \"impute_cautious\"\n",
    "        else: return \"consider_drop_or_advanced\"\n",
    "    elif tag == \"partial\":\n",
    "        if fmi < 0.20: return \"sensitivity_only\"\n",
    "        else: return \"exclude_from_FA\"\n",
    "    else:  # inconsistent\n",
    "        return \"exclude_from_FA\"\n",
    "\n",
    "decision_df[\"Action\"] = decision_df.apply(suggest_action, axis=1)\n",
    "\n",
    "# --- Reorder columns for clarity ---\n",
    "decision_df = decision_df[[\n",
    "    \"Variable\", \"ConsistencyTag\", \"OverallFMI\", \"Flag\",\n",
    "    \"Dimension\", \"Action\", \n",
    "]]\n",
    "\n",
    "# --- Save template ---\n",
    "out_file = DECISION_ROOT / \"Decision_Matrix.csv\"\n",
    "decision_df.to_csv(out_file, index=False)\n",
    "print(f\"[OK] Decision matrix template saved to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83e5fe",
   "metadata": {},
   "source": [
    "#### CRUCIAL NOTES (README)\n",
    "\n",
    "-  Not sure with the difference between `work indicator and work indicator.1.` Kindly see Decision_Matrix sheets for granular details.\n",
    "-  Also Check `Province and Province Recode` for missing values. Not sure what kind of imputation is applicable for this one since (assuming manual imputation, since lists of provinces can be acquired online and shall serve as a guide for encoding.). But we can still automate  this given that we have a strict list of dictionary once its acquired from online. IMPROPER IMPUTATION will done at this test stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255af06f",
   "metadata": {},
   "source": [
    "### Decision Matrix for Imputation - Defense\n",
    "\n",
    "This matrix is the bridge between FMI diagnostics and factor analysis.  \n",
    "It ensures that **every variable** is evaluated not only by its missingness (FMI) and consistency, but also by its **conceptual role** in financial vulnerability.\n",
    "\n",
    "- **Sensitivity**: Variables tied to employment stability, income regularity, and sectoral risk.  \n",
    "- **Resilience**: Variables reflecting household capacity, education, skills, and adaptability.  \n",
    "- **Exposure**: Variables representing structural or locational factors (region, province, urban/rural).\n",
    "\n",
    "#### Why automate?\n",
    "Manual factor formation was encoded into a reproducible dictionary and keyword rules.  \n",
    "This ensures consistency across runs, while still allowing customization:\n",
    "- The `dimension_map` dictionary can be edited to refine assignments.  \n",
    "- Keyword rules act as a fallback for variables not explicitly mapped.  \n",
    "- Any variable left as `\"Unclassified\"` is flagged for manual review.\n",
    "\n",
    "#### Why this is defensible?\n",
    "- **Theory-guided**: Dimensions are based on the approved thesis framework.  \n",
    "- **Transparent**: Every variable is listed, no silent exclusions.  \n",
    "- **Customizable**: Teammates can refine the dictionary or rationale column later.  \n",
    "- **Audit-ready**: The matrix documents not just FMI and consistency, but also conceptual relevance.\n",
    "\n",
    "This way, imputation decisions are **informed from the start**, but remain flexible for recalibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b73f6b",
   "metadata": {},
   "source": [
    "### Imputation Proper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223895e3",
   "metadata": {},
   "source": [
    "At this stage, basic imputation will be done to the missing values following the mentioned criterias above. This notebook is customizable according to the further rules that will further be applied to the analysis. For further context, kindly read the CRUCIAL NOTES (README) section in this notebook outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa72926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# --- Paths ---\n",
    "INPUT_ROOT = BASE_PATH / \"NEW Renamed Fully Decoded Surveys\"\n",
    "CONSISTENCY_ROOT = BASE_PATH / \"NEW Variable Consistency Check\"\n",
    "FMI_ROOT = BASE_PATH / \"NEW FMI Reports\"\n",
    "METADATA_ROOT = BASE_PATH / \"NEW Metadata Sheet 2 CSVs\"\n",
    "OUTPUT_ROOT = BASE_PATH / \"Imputed Data for Analysis\"\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load consistency + FMI profiles ---\n",
    "consistency_df = pd.read_csv(CONSISTENCY_ROOT / \"consistency_profile.csv\")\n",
    "fmi_df = pd.read_csv(FMI_ROOT / \"fmi_profile.csv\")\n",
    "\n",
    "decision_df = fmi_df.merge(\n",
    "    consistency_df[[\"Variable\", \"ConsistencyTag\"]],\n",
    "    on=\"Variable\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Deduplicate merge artifacts\n",
    "if \"ConsistencyTag_x\" in decision_df.columns and \"ConsistencyTag_y\" in decision_df.columns:\n",
    "    decision_df[\"ConsistencyTag\"] = decision_df[\"ConsistencyTag_x\"].combine_first(decision_df[\"ConsistencyTag_y\"])\n",
    "    decision_df.drop(columns=[\"ConsistencyTag_x\", \"ConsistencyTag_y\"], inplace=True)\n",
    "\n",
    "# --- Load metadata value sets ---\n",
    "metadata_dict = {}\n",
    "for file in Path(METADATA_ROOT).glob(\"*.csv\"):\n",
    "    meta_df = pd.read_csv(file)\n",
    "    if \"Variable\" in meta_df.columns and \"AllowedValues\" in meta_df.columns:\n",
    "        for _, row in meta_df.iterrows():\n",
    "            var = str(row[\"Variable\"]).strip()\n",
    "            values = str(row[\"AllowedValues\"]).split(\";\")\n",
    "            metadata_dict[var] = [v.strip().lower() for v in values if v.strip()]\n",
    "\n",
    "# --- Normalize names ---\n",
    "def normalize_name(name: str) -> str:\n",
    "    return (\n",
    "        str(name)\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace(\"\\xa0\", \" \")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"_\", \" \")\n",
    "    )\n",
    "\n",
    "decision_df[\"Variable_norm\"] = decision_df[\"Variable\"].apply(normalize_name)\n",
    "\n",
    "# --- Flexible finder with fuzzy matching ---\n",
    "def find_column(df, var):\n",
    "    cols_norm = {normalize_name(c): c for c in df.columns}\n",
    "    var_norm = normalize_name(var)\n",
    "\n",
    "    if var_norm in cols_norm:\n",
    "        return cols_norm[var_norm]\n",
    "\n",
    "    matches = get_close_matches(var_norm, list(cols_norm.keys()), n=1, cutoff=0.8)\n",
    "    if matches:\n",
    "        return cols_norm[matches[0]]\n",
    "\n",
    "    return None\n",
    "\n",
    "# --- Helpers ---\n",
    "def robust_mode(series: pd.Series):\n",
    "    m = series.mode(dropna=True)\n",
    "    return None if m.empty else m.iloc[0]\n",
    "\n",
    "def clean_age_column(col: pd.Series) -> pd.Series:\n",
    "    s = col.astype(str)\n",
    "    s = s.where(~s.str.contains(r\"\\d{4}-\\d{2}-\\d{2}\", regex=True), \"UnknownAge\")\n",
    "    numeric_coerced = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if numeric_coerced.notna().sum() >= (0.5 * len(s)):\n",
    "        return numeric_coerced.fillna(-1).astype(int)\n",
    "    else:\n",
    "        s = s.replace({\"nan\": \"UnknownAge\"})\n",
    "        return s\n",
    "\n",
    "# --- Metadata-guided flexible imputation ---\n",
    "def apply_imputation(df: pd.DataFrame, var: str, audit_rows: list):\n",
    "    col_name = find_column(df, var)\n",
    "    if col_name is None:\n",
    "        audit_rows.append({\n",
    "            \"Variable\": var,\n",
    "            \"MethodApplied\": \"not_matched\",\n",
    "            \"AllowedValues\": None,\n",
    "            \"BeforeMissing\": None,\n",
    "            \"AfterMissing\": None,\n",
    "            \"Note\": \"Variable not matched to any column (check naming).\"\n",
    "        })\n",
    "        return\n",
    "\n",
    "    # Normalize blanks to NaN\n",
    "    df[col_name] = df[col_name].replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "    before_missing = int(df[col_name].isna().sum())\n",
    "    dtype_numeric = pd.api.types.is_numeric_dtype(df[col_name])\n",
    "\n",
    "    allowed = metadata_dict.get(var, None)\n",
    "    method, note = \"none\", \"No imputation required.\"\n",
    "    after_missing = before_missing\n",
    "\n",
    "    if normalize_name(var) == normalize_name(\"C05-Age as of Last Birthday\"):\n",
    "        df[col_name] = clean_age_column(df[col_name])\n",
    "        dtype_numeric = pd.api.types.is_numeric_dtype(df[col_name])\n",
    "\n",
    "    if dtype_numeric:\n",
    "        if before_missing > 0:\n",
    "            med = df[col_name].median()\n",
    "            df[col_name].fillna(med, inplace=True)\n",
    "            method = \"median\"\n",
    "            note = f\"Numeric imputation with median={med:.4f}.\"\n",
    "            after_missing = int(df[col_name].isna().sum())\n",
    "    else:\n",
    "        if before_missing > 0:\n",
    "            mode_val = robust_mode(df[col_name])\n",
    "            if allowed:\n",
    "                # restrict mode to allowed values\n",
    "                if mode_val is not None and str(mode_val).lower() in allowed:\n",
    "                    df[col_name].fillna(mode_val, inplace=True)\n",
    "                    method = \"metadata_mode\"\n",
    "                    note = f\"Categorical imputation with mode='{mode_val}' (validated against metadata).\"\n",
    "                else:\n",
    "                    df[col_name].fillna(\"Unknown\", inplace=True)\n",
    "                    method = \"metadata_unknown\"\n",
    "                    note = \"No valid mode within metadata; filled with 'Unknown'.\"\n",
    "            else:\n",
    "                # fallback if no metadata\n",
    "                if mode_val is not None:\n",
    "                    df[col_name].fillna(mode_val, inplace=True)\n",
    "                    method = \"categorical_mode\"\n",
    "                    note = f\"Categorical imputation with mode='{mode_val}'.\"\n",
    "                else:\n",
    "                    df[col_name].fillna(\"Unknown\", inplace=True)\n",
    "                    method = \"unknown_fallback\"\n",
    "                    note = \"No valid mode; filled with 'Unknown'.\"\n",
    "            after_missing = int(df[col_name].isna().sum())\n",
    "\n",
    "    audit_rows.append({\n",
    "        \"Variable\": var,\n",
    "        \"MethodApplied\": method,\n",
    "        \"AllowedValues\": allowed,\n",
    "        \"BeforeMissing\": before_missing,\n",
    "        \"AfterMissing\": after_missing,\n",
    "        \"Note\": note\n",
    "    })\n",
    "\n",
    "# --- Year-by-year execution ---\n",
    "consistent_vars = consistency_df[consistency_df[\"ConsistencyTag\"] == \"consistent\"][\"Variable\"].tolist()\n",
    "\n",
    "for year_folder in INPUT_ROOT.iterdir():\n",
    "    if not year_folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    year_out_dir = OUTPUT_ROOT / year_folder.name\n",
    "    year_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file in year_folder.glob(\"*.csv\"):\n",
    "        print(f\"Processing {file.name} from {year_folder.name}\")\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # Normalize df columns\n",
    "        df.columns = [normalize_name(c) for c in df.columns]\n",
    "\n",
    "        # Audit log\n",
    "        audit_rows = []\n",
    "        for var in consistent_vars:\n",
    "            apply_imputation(df, var, audit_rows)\n",
    "\n",
    "        # Save imputed dataset\n",
    "        out_file = year_out_dir / f\"imputed_{file.stem}.csv\"\n",
    "        df.to_csv(out_file, index=False)\n",
    "\n",
    "        # Save audit log\n",
    "        audit_df = pd.DataFrame(audit_rows)\n",
    "        audit_file = year_out_dir / f\"imputation_log_{file.stem}.csv\"\n",
    "        audit_df.to_csv(audit_file, index=False)\n",
    "\n",
    "        print(f\"[OK] Saved {out_file} | Audit log: {audit_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2ca23",
   "metadata": {},
   "source": [
    "### Preprocessing and Imputation Pipeline\n",
    "\n",
    "**Column normalization**\n",
    "\n",
    "- All column names are standardized: lowercase, stripped of leading/trailing spaces, and harmonized by replacing dashes/underscores with spaces.\n",
    "\n",
    "- Fuzzy matching ensures Decision Matrix variables align with survey file headers, reducing mismatches across survey waves.\n",
    "\n",
    "**Missing value normalization**\n",
    "\n",
    "- Blanks and whitespace‑only entries are converted to NaN inline before imputation.\n",
    "- This guarantees that missingness is consistently recognized and that audit logs accurately reflect true counts.\n",
    "\n",
    "**Metadata‑guided imputation logic**\n",
    "\n",
    "- Consistent variables are always considered for imputation, even if flagged as consider_drop_or_advanced.\n",
    "- Allowed value sets are retrieved dynamically from NEW Metadata Sheet 2 CSVs to validate imputation choices.\n",
    "\n",
    "**Rules applied:**\n",
    "\n",
    "- Numeric variables: imputed with median; clipped to metadata‑defined ranges if available.\n",
    "\n",
    "- Binary categorical (≤3 allowed values): imputed with majority class (mode) validated against metadata.\n",
    "\n",
    "- General categorical: imputed with mode restricted to metadata values; fallback to \"Unknown\" if no valid mode exists.\n",
    "\n",
    "- Identifiers/time variables (e.g., PSU number, Survey Year): left unchanged to preserve structural integrity.\n",
    "\n",
    "- This design ensures imputations respect official metadata and avoid arbitrary category inflation.\n",
    "\n",
    "**Audit logging**\n",
    "\n",
    "- Each variable logs: Action, AllowedValues, MethodApplied, BeforeMissing, AfterMissing, and explanatory Note.\n",
    "\n",
    "-  Overrides are explicitly marked when imputation is applied to variables flagged as consider_drop_or_advanced.\n",
    "\n",
    "- Logs provide transparency across survey years and support reproducibility for thesis defense and team review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f49880",
   "metadata": {},
   "source": [
    "### Evaluation of Imputation (By Completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46e92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_ROOT = BASE_PATH / \"Imputed Data for Analysis\"\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for year_folder in OUTPUT_ROOT.iterdir():\n",
    "    if not year_folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    for file in year_folder.glob(\"imputed_*.csv\"):\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "        null_counts = df.isnull().sum()\n",
    "        total_missing = int(null_counts.sum())\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"Year\": year_folder.name,\n",
    "            \"File\": file.name,\n",
    "            \"TotalMissing\": total_missing,\n",
    "            \"Completeness\": \"PASS\" if total_missing == 0 else \"FAIL\",\n",
    "            **null_counts.to_dict()  # expand variable-level missing counts\n",
    "        })\n",
    "\n",
    "# Build DataFrame\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Preview file-level completeness\n",
    "print(summary_df[[\"Year\",\"File\",\"TotalMissing\",\"Completeness\"]])\n",
    "\n",
    "# Optional: Year-level summary\n",
    "year_summary = summary_df.groupby(\"Year\")[\"Completeness\"].value_counts().unstack(fill_value=0)\n",
    "print(\"\\nYear-level completeness summary:\")\n",
    "print(year_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad83af5",
   "metadata": {},
   "source": [
    "### Evaluation of Imputation (By Metadata Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10182d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata_results = []\n",
    "\n",
    "for year_folder in OUTPUT_ROOT.iterdir():\n",
    "    if not year_folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    for file in year_folder.glob(\"imputed_*.csv\"):\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "\n",
    "        for var, allowed in metadata_dict.items():\n",
    "            col_name = find_column(df, var)\n",
    "\n",
    "            # Case 1: Variable not found in file\n",
    "            if not col_name:\n",
    "                metadata_results.append({\n",
    "                    \"Year\": year_folder.name,\n",
    "                    \"File\": file.name,\n",
    "                    \"Variable\": var,\n",
    "                    \"AllowedValues\": allowed,\n",
    "                    \"InvalidValues\": [],\n",
    "                    \"Status\": \"NOT_FOUND\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Case 2: No metadata available\n",
    "            if not allowed:\n",
    "                metadata_results.append({\n",
    "                    \"Year\": year_folder.name,\n",
    "                    \"File\": file.name,\n",
    "                    \"Variable\": var,\n",
    "                    \"AllowedValues\": None,\n",
    "                    \"InvalidValues\": [],\n",
    "                    \"Status\": \"NO_METADATA\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Case 3: Validate against metadata\n",
    "            unique_vals = set(df[col_name].dropna().astype(str).str.lower())\n",
    "            valid_set = set([v.lower() for v in allowed] + [\"unknown\"])\n",
    "            invalid_vals = unique_vals - valid_set\n",
    "\n",
    "            metadata_results.append({\n",
    "                \"Year\": year_folder.name,\n",
    "                \"File\": file.name,\n",
    "                \"Variable\": var,\n",
    "                \"AllowedValues\": allowed,\n",
    "                \"InvalidValues\": list(invalid_vals),\n",
    "                \"Status\": \"OK\" if not invalid_vals else \"INVALID\"\n",
    "            })\n",
    "\n",
    "# Build DataFrame\n",
    "metadata_df = pd.DataFrame(metadata_results)\n",
    "\n",
    "# Preview first 20 rows\n",
    "print(metadata_df.head(20))\n",
    "\n",
    "# Summary counts\n",
    "print(\"\\nSummary by Status:\")\n",
    "print(metadata_df[\"Status\"].value_counts())\n",
    "\n",
    "# Optional: Year-level summary\n",
    "year_summary = metadata_df.groupby(\"Year\")[\"Status\"].value_counts().unstack(fill_value=0)\n",
    "print(\"\\nYear-level metadata accuracy summary:\")\n",
    "print(year_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
